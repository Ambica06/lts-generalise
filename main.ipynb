{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144502ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18846 documents\n",
      "\n",
      "Dataset shape: (18846, 3)\n",
      "\n",
      "Categories (20): ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Class distribution:\n",
      "true_label\n",
      "alt.atheism                 799\n",
      "comp.graphics               973\n",
      "comp.os.ms-windows.misc     985\n",
      "comp.sys.ibm.pc.hardware    982\n",
      "comp.sys.mac.hardware       963\n",
      "comp.windows.x              988\n",
      "misc.forsale                975\n",
      "rec.autos                   990\n",
      "rec.motorcycles             996\n",
      "rec.sport.baseball          994\n",
      "rec.sport.hockey            999\n",
      "sci.crypt                   991\n",
      "sci.electronics             984\n",
      "sci.med                     990\n",
      "sci.space                   987\n",
      "soc.religion.christian      997\n",
      "talk.politics.guns          910\n",
      "talk.politics.mideast       940\n",
      "talk.politics.misc          775\n",
      "talk.religion.misc          628\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n",
      "   document_id                                               text  \\\n",
      "0            0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...   \n",
      "1            1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...   \n",
      "2            2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...   \n",
      "3            3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...   \n",
      "4            4  From: Alexander Samuel McDiarmid <am2o+@andrew...   \n",
      "\n",
      "                 true_label  \n",
      "0          rec.sport.hockey  \n",
      "1  comp.sys.ibm.pc.hardware  \n",
      "2     talk.politics.mideast  \n",
      "3  comp.sys.ibm.pc.hardware  \n",
      "4     comp.sys.mac.hardware  \n",
      "\n",
      "Sample text from first document:\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killin\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Download the complete dataset (both train and test)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'document_id': range(len(newsgroups.data)),\n",
    "    'text': newsgroups.data,\n",
    "    'true_label': [newsgroups.target_names[label] for label in newsgroups.target]\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nCategories ({len(df['true_label'].unique())}): {sorted(df['true_label'].unique())}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['true_label'].value_counts().sort_index())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nSample text from first document:\")\n",
    "print(df['text'].iloc[0][:500])  # First 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6400d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA EXPLORATION AND INCONSISTENCY CHECK\n",
      "======================================================================\n",
      "\n",
      "1. BASIC STATISTICS:\n",
      "Total documents: 18846\n",
      "Number of categories: 20\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "2. NULL/EMPTY VALUES:\n",
      "document_id    0\n",
      "text           0\n",
      "true_label     0\n",
      "dtype: int64\n",
      "Empty strings: 0\n",
      "\n",
      "3. TEXT LENGTH DISTRIBUTION:\n",
      "count     18846.000000\n",
      "mean       1902.525894\n",
      "std        3984.970264\n",
      "min         115.000000\n",
      "25%         751.000000\n",
      "50%        1175.000000\n",
      "75%        1874.750000\n",
      "max      160616.000000\n",
      "Name: text_length, dtype: float64\n",
      "Documents with < 50 characters: 0\n",
      "Documents with < 100 characters: 0\n",
      "\n",
      "4. SAMPLE DOCUMENTS (First 3):\n",
      "\n",
      "--- Document 0 (Category: rec.sport.hockey) ---\n",
      "Length: 902 characters\n",
      "First 800 characters:\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was \n",
      "...\n",
      "\n",
      "--- Document 1 (Category: comp.sys.ibm.pc.hardware) ---\n",
      "Length: 963 characters\n",
      "First 800 characters:\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "-- \n",
      "    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \n",
      "  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  \n",
      "...\n",
      "\n",
      "--- Document 2 (Category: talk.politics.mideast) ---\n",
      "Length: 3780 characters\n",
      "First 800 characters:\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the\n",
      "|>Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>It has always been up to the Azeris to end their announced winning of Karabakh \n",
      "|>by removing the Armenians! When t\n",
      "...\n",
      "\n",
      "5. PATTERNS DETECTED:\n",
      "Documents with email headers: 18749 (99.5%)\n",
      "Documents with quoted text (>): 9486 (50.3%)\n",
      "Documents with email addresses: 18836 (99.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v3/r42_1hdn0nz7qkq861qvmsp00000gn/T/ipykernel_70464/3207241922.py:38: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  has_headers = df['text'].str.contains(r'^(From|Subject|Organization):', case=False, regex=True, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with URLs: 3 (0.0%)\n",
      "Documents with excessive whitespace: 15945 (84.6%)\n",
      "Documents with special characters: 18843 (100.0%)\n",
      "\n",
      "6. CLASS DISTRIBUTION:\n",
      "true_label\n",
      "alt.atheism                 799\n",
      "comp.graphics               973\n",
      "comp.os.ms-windows.misc     985\n",
      "comp.sys.ibm.pc.hardware    982\n",
      "comp.sys.mac.hardware       963\n",
      "comp.windows.x              988\n",
      "misc.forsale                975\n",
      "rec.autos                   990\n",
      "rec.motorcycles             996\n",
      "rec.sport.baseball          994\n",
      "rec.sport.hockey            999\n",
      "sci.crypt                   991\n",
      "sci.electronics             984\n",
      "sci.med                     990\n",
      "sci.space                   987\n",
      "soc.religion.christian      997\n",
      "talk.politics.guns          910\n",
      "talk.politics.mideast       940\n",
      "talk.politics.misc          775\n",
      "talk.religion.misc          628\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most common: 999 documents\n",
      "Least common: 628 documents\n",
      "Balance ratio (max/min): 1.59\n",
      "\n",
      "======================================================================\n",
      "EXPLORATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA EXPLORATION AND INCONSISTENCY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Basic statistics\n",
    "print(\"\\n1. BASIC STATISTICS:\")\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"Number of categories: {df['true_label'].nunique()}\")\n",
    "print(f\"Categories: {sorted(df['true_label'].unique())}\")\n",
    "\n",
    "# 2. Check for null/empty values\n",
    "print(\"\\n2. NULL/EMPTY VALUES:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"Empty strings: {(df['text'].str.strip() == '').sum()}\")\n",
    "\n",
    "# 3. Text length distribution\n",
    "print(\"\\n3. TEXT LENGTH DISTRIBUTION:\")\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(df['text_length'].describe())\n",
    "print(f\"Documents with < 50 characters: {(df['text_length'] < 50).sum()}\")\n",
    "print(f\"Documents with < 100 characters: {(df['text_length'] < 100).sum()}\")\n",
    "\n",
    "# 4. Sample a few documents to see their structure\n",
    "print(\"\\n4. SAMPLE DOCUMENTS (First 3):\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- Document {i} (Category: {df['true_label'].iloc[i]}) ---\")\n",
    "    print(f\"Length: {df['text_length'].iloc[i]} characters\")\n",
    "    print(f\"First 800 characters:\")\n",
    "    print(df['text'].iloc[i][:800])\n",
    "    print(\"...\")\n",
    "\n",
    "# 5. Check for common patterns that need cleaning\n",
    "print(\"\\n5. PATTERNS DETECTED:\")\n",
    "\n",
    "# Check for email headers\n",
    "has_headers = df['text'].str.contains(r'^(From|Subject|Organization):', case=False, regex=True, na=False)\n",
    "print(f\"Documents with email headers: {has_headers.sum()} ({has_headers.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for quoted text\n",
    "has_quotes = df['text'].str.contains(r'^>+', regex=True, flags=re.MULTILINE, na=False)\n",
    "print(f\"Documents with quoted text (>): {has_quotes.sum()} ({has_quotes.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for email addresses\n",
    "has_emails = df['text'].str.contains(r'\\S+@\\S+', regex=True, na=False)\n",
    "print(f\"Documents with email addresses: {has_emails.sum()} ({has_emails.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for URLs\n",
    "has_urls = df['text'].str.contains(r'http\\S+|www\\.\\S+', regex=True, na=False)\n",
    "print(f\"Documents with URLs: {has_urls.sum()} ({has_urls.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for excessive whitespace\n",
    "has_excess_space = df['text'].str.contains(r'\\s{3,}', regex=True, na=False)\n",
    "print(f\"Documents with excessive whitespace: {has_excess_space.sum()} ({has_excess_space.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for special characters\n",
    "has_special_chars = df['text'].str.contains(r'[^\\w\\s.,!?;:\\'\"()-]', regex=True, na=False)\n",
    "print(f\"Documents with special characters: {has_special_chars.sum()} ({has_special_chars.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 6. Class balance\n",
    "print(\"\\n6. CLASS DISTRIBUTION:\")\n",
    "class_counts = df['true_label'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print(f\"\\nMost common: {class_counts.max()} documents\")\n",
    "print(f\"Least common: {class_counts.min()} documents\")\n",
    "print(f\"Balance ratio (max/min): {class_counts.max()/class_counts.min():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2d6bdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning to all documents...\n",
      "\n",
      "======================================================================\n",
      "CLEANING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Text length comparison:\n",
      "Original:\n",
      "count     18846.000000\n",
      "mean       1902.525894\n",
      "std        3984.970264\n",
      "min         115.000000\n",
      "25%         751.000000\n",
      "50%        1175.000000\n",
      "75%        1874.750000\n",
      "max      160616.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Cleaned:\n",
      "count    18846.000000\n",
      "mean      1262.492041\n",
      "std       3367.285109\n",
      "min          0.000000\n",
      "25%        359.000000\n",
      "50%        627.500000\n",
      "75%       1143.750000\n",
      "max      82312.000000\n",
      "Name: cleaned_length, dtype: float64\n",
      "\n",
      "Documents with < 50 characters after cleaning: 164\n",
      "Documents with < 100 characters after cleaning: 533\n",
      "\n",
      "======================================================================\n",
      "BEFORE/AFTER EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "--- Document 0 (Category: rec.sport.hockey) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am goin\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "i am sure some bashers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils. actually, i am bit puzzled too and a bit relieved. however, i am going to put an end to non-pittsburghers' relief with a bit of praise for the pens. man, they are killing those devils worse than i thought. jagr just showed you why he is much better than his regu\n",
      "\n",
      "Length: 902 → 698\n",
      "\n",
      "--- Document 1 (Category: comp.sys.ibm.pc.hardware) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VE\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "my brother is in the market for a high-performance video card that supports vesa local bus with 1-2mb ram. does anyone have suggestions/ideas on: - diamond stealth pro local bus - orchid farenheit 1280 - ati graphics ultra pro - any other high-performance vlb card please post or email. thank you! - matt -- | matthew b. lawson <------------> | --+-- \"now i, nebuchadnezzar, praise and exalt and glor\n",
      "\n",
      "Length: 963 → 545\n",
      "\n",
      "--- Document 2 (Category: talk.politics.mideast) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stre\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "|>the student of \"regional killings\" alias davidian (not the davidian religios sect) writes: |>greater armenia would stretch from karabakh, to the black sea, to the |>mediterranean, so if you use the term \"greater armenia\" use it with care. finally you said what you dream about. mediterranean???? that was new.... the area will be \"greater\" after some years, like your \"holocaust\" numbers...... |>it\n",
      "\n",
      "Length: 3780 → 3282\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET\n",
      "======================================================================\n",
      "Original documents: 18846\n",
      "After cleaning and filtering (min_length=50): 18682\n",
      "Removed: 164 documents\n",
      "\n",
      "Final shape: (18682, 3)\n",
      "\n",
      "Sample of final dataset:\n",
      "   document_id                                               text  \\\n",
      "0            0  i am sure some bashers of pens fans are pretty...   \n",
      "1            1  my brother is in the market for a high-perform...   \n",
      "2            2  |>the student of \"regional killings\" alias dav...   \n",
      "3            3  in article (wayne smith) writes: think! it's t...   \n",
      "4            4  1) i have an old jasmine drive which i cannot ...   \n",
      "\n",
      "                 true_label  \n",
      "0          rec.sport.hockey  \n",
      "1  comp.sys.ibm.pc.hardware  \n",
      "2     talk.politics.mideast  \n",
      "3  comp.sys.ibm.pc.hardware  \n",
      "4     comp.sys.mac.hardware  \n"
     ]
    }
   ],
   "source": [
    "def clean_newsgroup_text(text):\n",
    "    \"\"\"Clean newsgroup text based on identified patterns.\"\"\"\n",
    "    \n",
    "    # 1. Remove email headers (lines starting with common patterns)\n",
    "    # Common headers: From:, Subject:, Organization:, Lines:, NNTP-Posting-Host:, etc.\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    in_header = True\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip header lines (lines with key: value format at the start)\n",
    "        if in_header and re.match(r'^[\\w-]+:', line):\n",
    "            continue\n",
    "        elif in_header and line.strip() == '':\n",
    "            # Empty line often marks end of headers\n",
    "            in_header = False\n",
    "            continue\n",
    "        else:\n",
    "            in_header = False\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # 2. Remove quoted text (lines starting with > or >>)\n",
    "    text = re.sub(r'^>+.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove URLs (minimal presence but good to clean)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 5. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 6. Normalize whitespace (replace multiple spaces/newlines with single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Applying cleaning to all documents...\")\n",
    "df['text_cleaned'] = df['text'].apply(clean_newsgroup_text)\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate new text lengths\n",
    "df['cleaned_length'] = df['text_cleaned'].str.len()\n",
    "\n",
    "print(\"\\nText length comparison:\")\n",
    "print(\"Original:\")\n",
    "print(df['text_length'].describe())\n",
    "print(\"\\nCleaned:\")\n",
    "print(df['cleaned_length'].describe())\n",
    "\n",
    "# Check for very short documents after cleaning\n",
    "print(f\"\\nDocuments with < 50 characters after cleaning: {(df['cleaned_length'] < 50).sum()}\")\n",
    "print(f\"Documents with < 100 characters after cleaning: {(df['cleaned_length'] < 100).sum()}\")\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEFORE/AFTER EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    print(f\"\\n--- Document {i} (Category: {df['true_label'].iloc[i]}) ---\")\n",
    "    print(f\"\\nORIGINAL (first 400 chars):\")\n",
    "    print(df['text'].iloc[i][:400])\n",
    "    print(f\"\\nCLEANED (first 400 chars):\")\n",
    "    print(df['text_cleaned'].iloc[i][:400])\n",
    "    print(f\"\\nLength: {df['text_length'].iloc[i]} → {df['cleaned_length'].iloc[i]}\")\n",
    "\n",
    "# Filter out very short documents (optional - set threshold)\n",
    "min_length = 50\n",
    "df_final = df[df['cleaned_length'] >= min_length].copy()\n",
    "\n",
    "# Reset document IDs and keep only necessary columns\n",
    "df_final = df_final[['true_label', 'text_cleaned']].reset_index(drop=True)\n",
    "df_final['document_id'] = range(len(df_final))\n",
    "df_final = df_final[['document_id', 'text_cleaned', 'true_label']]\n",
    "df_final.rename(columns={'text_cleaned': 'text'}, inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original documents: {len(df)}\")\n",
    "print(f\"After cleaning and filtering (min_length={min_length}): {len(df_final)}\")\n",
    "print(f\"Removed: {len(df) - len(df_final)} documents\")\n",
    "print(f\"\\nFinal shape: {df_final.shape}\")\n",
    "print(f\"\\nSample of final dataset:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596ec04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "def generate_and_save_embeddings(df, dataset_name, column_name='text',\n",
    "                                 model_name='all-MiniLM-L6-v2',\n",
    "                                 batch_size=32,\n",
    "                                 device='cpu',\n",
    "                                 save_dir='embeddings'):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate embeddings for a dataframe column using a pre-trained SentenceTransformer model\n",
    "    and save the embeddings to a .npy file.\n",
    "    \n",
    "    Returns:\n",
    "    - embeddings: numpy array of embeddings\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    safe_name = dataset_name.replace(\" \", \"_\").lower()\n",
    "    save_path = os.path.join(save_dir, f\"{safe_name}_embeddings.npy\")\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        df[column_name].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    np.save(save_path, embeddings)\n",
    "\n",
    "    return embeddings, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666aeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def lts_kmeans_sampling(df, embeddings, k,\n",
    "                        cluster_col='cluster',\n",
    "                        random_state=42,\n",
    "                        n_init=10,\n",
    "                        max_iter=300,\n",
    "                        output_csv='sampled_documents_lts.csv'):\n",
    "    \"\"\"\n",
    "    Perform LTS-style k-means clustering and select one representative\n",
    "    document per cluster (closest to centroid).\n",
    "\n",
    "    Returns:\n",
    "    - sampled_df: pandas DataFrame containing sampled documents\n",
    "    - sampled_indices: list of selected indices\n",
    "    - kmeans: fitted KMeans model\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Running k-means with k={k} clusters...\")\n",
    "\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=random_state,\n",
    "        n_init=n_init,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Add cluster assignments to dataframe\n",
    "    df = df.copy()\n",
    "    df[cluster_col] = cluster_labels\n",
    "\n",
    "    sampled_indices = []\n",
    "\n",
    "    for cluster_id in range(k):\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "        centroid = kmeans.cluster_centers_[cluster_id]\n",
    "\n",
    "        distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
    "        closest_idx = cluster_indices[np.argmin(distances)]\n",
    "        sampled_indices.append(closest_idx)\n",
    "\n",
    "    sampled_df = df.iloc[sampled_indices].copy()\n",
    "\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"Sampled dataset size: {len(sampled_df)}\")\n",
    "    print(f\"Reduction: {len(sampled_df) / len(df) * 100:.1f}%\")\n",
    "\n",
    "    sampled_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return sampled_df, sampled_indices, kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b560014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fb89ca58224eb584a5ce61f46ac9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings, path = generate_and_save_embeddings(df, dataset_name=\"news_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2155c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-means with k=100 clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 18846\n",
      "Sampled dataset size: 100\n",
      "Reduction: 0.5%\n"
     ]
    }
   ],
   "source": [
    "sampled_df, sampled_indices, kmeans = lts_kmeans_sampling(\n",
    "    df,\n",
    "    embeddings,\n",
    "    k=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69d06b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12800</td>\n",
       "      <td>From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>948</td>\n",
       "      <td>someone please buy these books!!!!! i am not a...</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10497</td>\n",
       "      <td>From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>1831</td>\n",
       "      <td>in article () writes: hey, joe -- assuming you...</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9913</td>\n",
       "      <td>From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>2303</td>\n",
       "      <td>(joseph b stiehm) writes: as if an aluminum st...</td>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6094</td>\n",
       "      <td>From: sfp@lemur.cit.cornell.edu (Sheila Patter...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>2419</td>\n",
       "      <td>in article (anni dozier) writes: |&gt; after read...</td>\n",
       "      <td>2155</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10250</td>\n",
       "      <td>From: glang@slee01.srl.ford.com (Gordon Lang)\\...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>1331</td>\n",
       "      <td>volker voecking wrote: : : hello : : i have pr...</td>\n",
       "      <td>1069</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text  \\\n",
       "0        12800  From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...   \n",
       "1        10497  From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...   \n",
       "2         9913  From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...   \n",
       "3         6094  From: sfp@lemur.cit.cornell.edu (Sheila Patter...   \n",
       "4        10250  From: glang@slee01.srl.ford.com (Gordon Lang)\\...   \n",
       "\n",
       "                 true_label  text_length  \\\n",
       "0              misc.forsale          948   \n",
       "1        talk.politics.guns         1831   \n",
       "2          rec.sport.hockey         2303   \n",
       "3    soc.religion.christian         2419   \n",
       "4  comp.sys.ibm.pc.hardware         1331   \n",
       "\n",
       "                                        text_cleaned  cleaned_length  cluster  \n",
       "0  someone please buy these books!!!!! i am not a...             696        0  \n",
       "1  in article () writes: hey, joe -- assuming you...             578        1  \n",
       "2  (joseph b stiehm) writes: as if an aluminum st...             869        2  \n",
       "3  in article (anni dozier) writes: |> after read...            2155        3  \n",
       "4  volker voecking wrote: : : hello : : i have pr...            1069        4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM Labelling starts here\n",
    "import pandas as pd\n",
    "sampled_df = pd.read_csv('sampled_documents_lts.csv')\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94b40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# We chose OpenAI, a cloud LLM, to assist us with the LLM labelling\n",
    "# We retrieved the API key from the link: https://platform.openai.com/api-keys\n",
    "import sys\n",
    "\n",
    "# Install/upgrade the OpenAI Python client in the SAME environment Jupyter is using\n",
    "!{sys.executable} -m pip install --quiet \"openai>=1.0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffbf626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "# OpenAI couldn't originally be founded, so I was trying to identify the path for the active environment\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7326f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 2.9.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f075ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User site-packages: /Users/ambica/.local/lib/python3.12/site-packages\n",
      "openai in path? False\n"
     ]
    }
   ],
   "source": [
    "# Tryingt to get Jupyter-notebook to use the OpenAI LLM model\n",
    "import site, sys\n",
    "\n",
    "# Make sure Python can see packages installed in ~/.local/lib/python3.11/site-packages\n",
    "user_site = site.getusersitepackages()\n",
    "if user_site not in sys.path:\n",
    "    sys.path.append(user_site)\n",
    "\n",
    "print(\"User site-packages:\", user_site)\n",
    "print(\"openai in path?\", any(\"openai\" in p for p in sys.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d246f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Testing the import, since it outputs: OpenAI version: 2.9.0, it means that it works now\n",
    "\n",
    "import openai\n",
    "print(\"OpenAI version:\", openai.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3125362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d40d180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling using a schema that categorizes Topic, Content Intent, and Sentiment (only one will be chosen from each of the 3 categories)\n",
    "import json\n",
    "\n",
    "def classify_text_openai(text: str) -> dict:\n",
    "    prompt = f\"\"\"You are a careful annotator for a news/discussion dataset.\n",
    "Classify the following text along THREE axes: Topic, Content Intent, and Sentiment.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "1) TOPIC (choose exactly ONE):\n",
    "- Technology & Computers\n",
    "- Science & Engineering\n",
    "- Sports & Recreation\n",
    "- Politics & Law\n",
    "- Religion & Philosophy\n",
    "- Business & Finance\n",
    "- Personal / Culture / Other\n",
    "\n",
    "2) CONTENT INTENT (choose exactly ONE):\n",
    "- Inform\n",
    "- Persuade\n",
    "- Entertain\n",
    "- Express personal feelings\n",
    "- Ask a question\n",
    "- Other\n",
    "\n",
    "3) SENTIMENT (choose exactly ONE):\n",
    "- Positive\n",
    "- Neutral\n",
    "- Negative\n",
    "\n",
    "Respond ONLY in valid JSON using this exact format:\n",
    "{{\n",
    "  \"topic\": \"...\",\n",
    "  \"intent\": \"...\",\n",
    "  \"sentiment\": \"...\"\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"topic\": \"Personal / Culture / Other\",\n",
    "            \"intent\": \"Other\",\n",
    "            \"sentiment\": \"Neutral\",\n",
    "            \"error\": str(e),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8901584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling 100/100...\n",
      "Done! Labeled: 100, Errors: 100\n"
     ]
    }
   ],
   "source": [
    "labels_cloud = []\n",
    "error_count_cloud = 0\n",
    "\n",
    "texts = sampled_df[\"text\"].astype(str).tolist()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Labelling {i+1}/{len(texts)}...\", end=\"\\r\")\n",
    "    result = classify_text_openai(text)\n",
    "    if \"error\" in result:\n",
    "        error_count_cloud += 1\n",
    "    labels_cloud.append(result)\n",
    "\n",
    "print(f\"\\nDone! Labeled: {len(labels_cloud)}, Errors: {error_count_cloud}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647f7bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved labeled data to: sampled_documents_lts_labeled_openai.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>cluster</th>\n",
       "      <th>topic</th>\n",
       "      <th>intent</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12800</td>\n",
       "      <td>From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>948</td>\n",
       "      <td>someone please buy these books!!!!! i am not a...</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>Personal / Culture / Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10497</td>\n",
       "      <td>From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>1831</td>\n",
       "      <td>in article () writes: hey, joe -- assuming you...</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>Personal / Culture / Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9913</td>\n",
       "      <td>From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>2303</td>\n",
       "      <td>(joseph b stiehm) writes: as if an aluminum st...</td>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "      <td>Personal / Culture / Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6094</td>\n",
       "      <td>From: sfp@lemur.cit.cornell.edu (Sheila Patter...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>2419</td>\n",
       "      <td>in article (anni dozier) writes: |&gt; after read...</td>\n",
       "      <td>2155</td>\n",
       "      <td>3</td>\n",
       "      <td>Personal / Culture / Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10250</td>\n",
       "      <td>From: glang@slee01.srl.ford.com (Gordon Lang)\\...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>1331</td>\n",
       "      <td>volker voecking wrote: : : hello : : i have pr...</td>\n",
       "      <td>1069</td>\n",
       "      <td>4</td>\n",
       "      <td>Personal / Culture / Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text  \\\n",
       "0        12800  From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...   \n",
       "1        10497  From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...   \n",
       "2         9913  From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...   \n",
       "3         6094  From: sfp@lemur.cit.cornell.edu (Sheila Patter...   \n",
       "4        10250  From: glang@slee01.srl.ford.com (Gordon Lang)\\...   \n",
       "\n",
       "                 true_label  text_length  \\\n",
       "0              misc.forsale          948   \n",
       "1        talk.politics.guns         1831   \n",
       "2          rec.sport.hockey         2303   \n",
       "3    soc.religion.christian         2419   \n",
       "4  comp.sys.ibm.pc.hardware         1331   \n",
       "\n",
       "                                        text_cleaned  cleaned_length  cluster  \\\n",
       "0  someone please buy these books!!!!! i am not a...             696        0   \n",
       "1  in article () writes: hey, joe -- assuming you...             578        1   \n",
       "2  (joseph b stiehm) writes: as if an aluminum st...             869        2   \n",
       "3  in article (anni dozier) writes: |> after read...            2155        3   \n",
       "4  volker voecking wrote: : : hello : : i have pr...            1069        4   \n",
       "\n",
       "                        topic intent sentiment  \n",
       "0  Personal / Culture / Other  Other   Neutral  \n",
       "1  Personal / Culture / Other  Other   Neutral  \n",
       "2  Personal / Culture / Other  Other   Neutral  \n",
       "3  Personal / Culture / Other  Other   Neutral  \n",
       "4  Personal / Culture / Other  Other   Neutral  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the labelled dataset as a separate csv file\n",
    "\n",
    "sampled_df[\"topic\"] = [r.get(\"topic\", \"Personal / Culture / Other\") for r in labels_cloud]\n",
    "sampled_df[\"intent\"] = [r.get(\"intent\", \"Other\") for r in labels_cloud]\n",
    "sampled_df[\"sentiment\"] = [r.get(\"sentiment\", \"Neutral\") for r in labels_cloud]\n",
    "\n",
    "# Adding the labels to the last columns\n",
    "ordered_cols = [\n",
    "    \"document_id\",\n",
    "    \"text\",\n",
    "    \"true_label\",\n",
    "    \"text_length\",\n",
    "    \"text_cleaned\",\n",
    "    \"cleaned_length\",\n",
    "    \"cluster\",\n",
    "    \"topic\",\n",
    "    \"intent\",\n",
    "    \"sentiment\",\n",
    "]\n",
    "\n",
    "sampled_df = sampled_df[ordered_cols]\n",
    "\n",
    "# Save the output file\n",
    "output_path = \"sampled_documents_lts_labeled_openai.csv\"\n",
    "sampled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved labeled data to: {output_path}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf8d7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>topic</th>\n",
       "      <th>intent</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>Inform</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>Politics &amp; Law</td>\n",
       "      <td>Persuade</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Sports &amp; Recreation</td>\n",
       "      <td>Entertain</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>Religion &amp; Philosophy</td>\n",
       "      <td>Persuade</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>Technology &amp; Computers</td>\n",
       "      <td>Inform</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 true_label                   topic     intent sentiment\n",
       "0              misc.forsale      Business & Finance     Inform   Neutral\n",
       "1        talk.politics.guns          Politics & Law   Persuade  Negative\n",
       "2          rec.sport.hockey     Sports & Recreation  Entertain   Neutral\n",
       "3    soc.religion.christian   Religion & Philosophy   Persuade   Neutral\n",
       "4  comp.sys.ibm.pc.hardware  Technology & Computers     Inform   Neutral"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def map_topic(label: str) -> str:\n",
    "    if re.match(r\"sci\\.\", label):\n",
    "        return \"Science & Engineering\"\n",
    "    if re.match(r\"comp\\.\", label):\n",
    "        return \"Technology & Computers\"\n",
    "    if re.match(r\"rec\\.\", label):\n",
    "        return \"Sports & Recreation\"\n",
    "    if re.match(r\"talk\\.politics\", label):\n",
    "        return \"Politics & Law\"\n",
    "    if re.match(r\"(soc\\.religion|talk\\.religion)\", label):\n",
    "        return \"Religion & Philosophy\"\n",
    "    if label == \"misc.forsale\":\n",
    "        return \"Business & Finance\"\n",
    "    return \"Personal / Culture / Other\"\n",
    "\n",
    "def map_intent(label: str) -> str:\n",
    "    # tech/science groups usually informational\n",
    "    if re.match(r\"(sci|comp)\\.\", label):\n",
    "        return \"Inform\"\n",
    "    # politics / religion usually persuasive or opinionated\n",
    "    if re.match(r\"(talk\\.politics|soc\\.religion|talk\\.religion)\", label):\n",
    "        return \"Persuade\"\n",
    "    # rec.* mostly recreational / entertaining\n",
    "    if re.match(r\"rec\\.\", label):\n",
    "        return \"Entertain\"\n",
    "    if label == \"misc.forsale\":\n",
    "        return \"Inform\"\n",
    "    # everything else: general discussion / opinions\n",
    "    return \"Express personal feelings\"\n",
    "\n",
    "def map_sentiment(label: str) -> str:\n",
    "    # politics tends to be heated; we flag it as negative by default\n",
    "    if re.match(r\"talk\\.politics\", label):\n",
    "        return \"Negative\"\n",
    "    # everything else default to neutral\n",
    "    return \"Neutral\"\n",
    "\n",
    "# Create / overwrite the three columns from true_label\n",
    "sampled_df[\"topic\"] = sampled_df[\"true_label\"].apply(map_topic)\n",
    "sampled_df[\"intent\"] = sampled_df[\"true_label\"].apply(map_intent)\n",
    "sampled_df[\"sentiment\"] = sampled_df[\"true_label\"].apply(map_sentiment)\n",
    "\n",
    "sampled_df[[\"true_label\", \"topic\", \"intent\", \"sentiment\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3935b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved labeled data to: sampled_documents_lts_labeled_rulebased.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>cluster</th>\n",
       "      <th>topic</th>\n",
       "      <th>intent</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12800</td>\n",
       "      <td>From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>948</td>\n",
       "      <td>someone please buy these books!!!!! i am not a...</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>Inform</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10497</td>\n",
       "      <td>From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>1831</td>\n",
       "      <td>in article () writes: hey, joe -- assuming you...</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics &amp; Law</td>\n",
       "      <td>Persuade</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9913</td>\n",
       "      <td>From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>2303</td>\n",
       "      <td>(joseph b stiehm) writes: as if an aluminum st...</td>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "      <td>Sports &amp; Recreation</td>\n",
       "      <td>Entertain</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6094</td>\n",
       "      <td>From: sfp@lemur.cit.cornell.edu (Sheila Patter...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>2419</td>\n",
       "      <td>in article (anni dozier) writes: |&gt; after read...</td>\n",
       "      <td>2155</td>\n",
       "      <td>3</td>\n",
       "      <td>Religion &amp; Philosophy</td>\n",
       "      <td>Persuade</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10250</td>\n",
       "      <td>From: glang@slee01.srl.ford.com (Gordon Lang)\\...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>1331</td>\n",
       "      <td>volker voecking wrote: : : hello : : i have pr...</td>\n",
       "      <td>1069</td>\n",
       "      <td>4</td>\n",
       "      <td>Technology &amp; Computers</td>\n",
       "      <td>Inform</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text  \\\n",
       "0        12800  From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...   \n",
       "1        10497  From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...   \n",
       "2         9913  From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...   \n",
       "3         6094  From: sfp@lemur.cit.cornell.edu (Sheila Patter...   \n",
       "4        10250  From: glang@slee01.srl.ford.com (Gordon Lang)\\...   \n",
       "\n",
       "                 true_label  text_length  \\\n",
       "0              misc.forsale          948   \n",
       "1        talk.politics.guns         1831   \n",
       "2          rec.sport.hockey         2303   \n",
       "3    soc.religion.christian         2419   \n",
       "4  comp.sys.ibm.pc.hardware         1331   \n",
       "\n",
       "                                        text_cleaned  cleaned_length  cluster  \\\n",
       "0  someone please buy these books!!!!! i am not a...             696        0   \n",
       "1  in article () writes: hey, joe -- assuming you...             578        1   \n",
       "2  (joseph b stiehm) writes: as if an aluminum st...             869        2   \n",
       "3  in article (anni dozier) writes: |> after read...            2155        3   \n",
       "4  volker voecking wrote: : : hello : : i have pr...            1069        4   \n",
       "\n",
       "                    topic     intent sentiment  \n",
       "0      Business & Finance     Inform   Neutral  \n",
       "1          Politics & Law   Persuade  Negative  \n",
       "2     Sports & Recreation  Entertain   Neutral  \n",
       "3   Religion & Philosophy   Persuade   Neutral  \n",
       "4  Technology & Computers     Inform   Neutral  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desired column order: original columns + new labels\n",
    "ordered_cols = [\n",
    "    \"document_id\",\n",
    "    \"text\",\n",
    "    \"true_label\",\n",
    "    \"text_length\",\n",
    "    \"text_cleaned\",\n",
    "    \"cleaned_length\",\n",
    "    \"cluster\",\n",
    "    \"topic\",\n",
    "    \"intent\",\n",
    "    \"sentiment\",\n",
    "]\n",
    "\n",
    "sampled_df = sampled_df[ordered_cols]\n",
    "\n",
    "output_path = \"sampled_documents_lts_labeled_rulebased.csv\"\n",
    "sampled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved labeled data to: {output_path}\")\n",
    "sampled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28754c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

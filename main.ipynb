{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144502ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Download the complete dataset (both train and test)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'document_id': range(len(newsgroups.data)),\n",
    "    'text': newsgroups.data,\n",
    "    'true_label': [newsgroups.target_names[label] for label in newsgroups.target]\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nCategories ({len(df['true_label'].unique())}): {sorted(df['true_label'].unique())}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['true_label'].value_counts().sort_index())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nSample text from first document:\")\n",
    "print(df['text'].iloc[0][:500])  # First 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6400d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA EXPLORATION AND INCONSISTENCY CHECK\n",
      "======================================================================\n",
      "\n",
      "1. BASIC STATISTICS:\n",
      "Total documents: 18846\n",
      "Number of categories: 20\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "2. NULL/EMPTY VALUES:\n",
      "document_id    0\n",
      "text           0\n",
      "true_label     0\n",
      "dtype: int64\n",
      "Empty strings: 0\n",
      "\n",
      "3. TEXT LENGTH DISTRIBUTION:\n",
      "count     18846.000000\n",
      "mean       1902.525894\n",
      "std        3984.970264\n",
      "min         115.000000\n",
      "25%         751.000000\n",
      "50%        1175.000000\n",
      "75%        1874.750000\n",
      "max      160616.000000\n",
      "Name: text_length, dtype: float64\n",
      "Documents with < 50 characters: 0\n",
      "Documents with < 100 characters: 0\n",
      "\n",
      "4. SAMPLE DOCUMENTS (First 3):\n",
      "\n",
      "--- Document 0 (Category: rec.sport.hockey) ---\n",
      "Length: 902 characters\n",
      "First 800 characters:\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was \n",
      "...\n",
      "\n",
      "--- Document 1 (Category: comp.sys.ibm.pc.hardware) ---\n",
      "Length: 963 characters\n",
      "First 800 characters:\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "-- \n",
      "    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \n",
      "  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  \n",
      "...\n",
      "\n",
      "--- Document 2 (Category: talk.politics.mideast) ---\n",
      "Length: 3780 characters\n",
      "First 800 characters:\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the\n",
      "|>Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>It has always been up to the Azeris to end their announced winning of Karabakh \n",
      "|>by removing the Armenians! When t\n",
      "...\n",
      "\n",
      "5. PATTERNS DETECTED:\n",
      "Documents with email headers: 18749 (99.5%)\n",
      "Documents with quoted text (>): 9486 (50.3%)\n",
      "Documents with email addresses: 18836 (99.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v3/r42_1hdn0nz7qkq861qvmsp00000gn/T/ipykernel_93689/3207241922.py:38: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  has_headers = df['text'].str.contains(r'^(From|Subject|Organization):', case=False, regex=True, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with URLs: 3 (0.0%)\n",
      "Documents with excessive whitespace: 15945 (84.6%)\n",
      "Documents with special characters: 18843 (100.0%)\n",
      "\n",
      "6. CLASS DISTRIBUTION:\n",
      "true_label\n",
      "alt.atheism                 799\n",
      "comp.graphics               973\n",
      "comp.os.ms-windows.misc     985\n",
      "comp.sys.ibm.pc.hardware    982\n",
      "comp.sys.mac.hardware       963\n",
      "comp.windows.x              988\n",
      "misc.forsale                975\n",
      "rec.autos                   990\n",
      "rec.motorcycles             996\n",
      "rec.sport.baseball          994\n",
      "rec.sport.hockey            999\n",
      "sci.crypt                   991\n",
      "sci.electronics             984\n",
      "sci.med                     990\n",
      "sci.space                   987\n",
      "soc.religion.christian      997\n",
      "talk.politics.guns          910\n",
      "talk.politics.mideast       940\n",
      "talk.politics.misc          775\n",
      "talk.religion.misc          628\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most common: 999 documents\n",
      "Least common: 628 documents\n",
      "Balance ratio (max/min): 1.59\n",
      "\n",
      "======================================================================\n",
      "EXPLORATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA EXPLORATION AND INCONSISTENCY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Basic statistics\n",
    "print(\"\\n1. BASIC STATISTICS:\")\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"Number of categories: {df['true_label'].nunique()}\")\n",
    "print(f\"Categories: {sorted(df['true_label'].unique())}\")\n",
    "\n",
    "# 2. Check for null/empty values\n",
    "print(\"\\n2. NULL/EMPTY VALUES:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"Empty strings: {(df['text'].str.strip() == '').sum()}\")\n",
    "\n",
    "# 3. Text length distribution\n",
    "print(\"\\n3. TEXT LENGTH DISTRIBUTION:\")\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(df['text_length'].describe())\n",
    "print(f\"Documents with < 50 characters: {(df['text_length'] < 50).sum()}\")\n",
    "print(f\"Documents with < 100 characters: {(df['text_length'] < 100).sum()}\")\n",
    "\n",
    "# 4. Sample a few documents to see their structure\n",
    "print(\"\\n4. SAMPLE DOCUMENTS (First 3):\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- Document {i} (Category: {df['true_label'].iloc[i]}) ---\")\n",
    "    print(f\"Length: {df['text_length'].iloc[i]} characters\")\n",
    "    print(f\"First 800 characters:\")\n",
    "    print(df['text'].iloc[i][:800])\n",
    "    print(\"...\")\n",
    "\n",
    "# 5. Check for common patterns that need cleaning\n",
    "print(\"\\n5. PATTERNS DETECTED:\")\n",
    "\n",
    "# Check for email headers\n",
    "has_headers = df['text'].str.contains(r'^(From|Subject|Organization):', case=False, regex=True, na=False)\n",
    "print(f\"Documents with email headers: {has_headers.sum()} ({has_headers.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for quoted text\n",
    "has_quotes = df['text'].str.contains(r'^>+', regex=True, flags=re.MULTILINE, na=False)\n",
    "print(f\"Documents with quoted text (>): {has_quotes.sum()} ({has_quotes.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for email addresses\n",
    "has_emails = df['text'].str.contains(r'\\S+@\\S+', regex=True, na=False)\n",
    "print(f\"Documents with email addresses: {has_emails.sum()} ({has_emails.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for URLs\n",
    "has_urls = df['text'].str.contains(r'http\\S+|www\\.\\S+', regex=True, na=False)\n",
    "print(f\"Documents with URLs: {has_urls.sum()} ({has_urls.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for excessive whitespace\n",
    "has_excess_space = df['text'].str.contains(r'\\s{3,}', regex=True, na=False)\n",
    "print(f\"Documents with excessive whitespace: {has_excess_space.sum()} ({has_excess_space.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for special characters\n",
    "has_special_chars = df['text'].str.contains(r'[^\\w\\s.,!?;:\\'\"()-]', regex=True, na=False)\n",
    "print(f\"Documents with special characters: {has_special_chars.sum()} ({has_special_chars.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 6. Class balance\n",
    "print(\"\\n6. CLASS DISTRIBUTION:\")\n",
    "class_counts = df['true_label'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print(f\"\\nMost common: {class_counts.max()} documents\")\n",
    "print(f\"Least common: {class_counts.min()} documents\")\n",
    "print(f\"Balance ratio (max/min): {class_counts.max()/class_counts.min():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e2d6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning to all documents...\n",
      "\n",
      "======================================================================\n",
      "CLEANING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Text length comparison:\n",
      "Original:\n",
      "count     18846.000000\n",
      "mean       1902.525894\n",
      "std        3984.970264\n",
      "min         115.000000\n",
      "25%         751.000000\n",
      "50%        1175.000000\n",
      "75%        1874.750000\n",
      "max      160616.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Cleaned:\n",
      "count    18846.000000\n",
      "mean      1262.492041\n",
      "std       3367.285109\n",
      "min          0.000000\n",
      "25%        359.000000\n",
      "50%        627.500000\n",
      "75%       1143.750000\n",
      "max      82312.000000\n",
      "Name: cleaned_length, dtype: float64\n",
      "\n",
      "Documents with < 50 characters after cleaning: 164\n",
      "Documents with < 100 characters after cleaning: 533\n",
      "\n",
      "======================================================================\n",
      "BEFORE/AFTER EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "--- Document 0 (Category: rec.sport.hockey) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am goin\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "i am sure some bashers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils. actually, i am bit puzzled too and a bit relieved. however, i am going to put an end to non-pittsburghers' relief with a bit of praise for the pens. man, they are killing those devils worse than i thought. jagr just showed you why he is much better than his regu\n",
      "\n",
      "Length: 902 → 698\n",
      "\n",
      "--- Document 1 (Category: comp.sys.ibm.pc.hardware) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VE\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "my brother is in the market for a high-performance video card that supports vesa local bus with 1-2mb ram. does anyone have suggestions/ideas on: - diamond stealth pro local bus - orchid farenheit 1280 - ati graphics ultra pro - any other high-performance vlb card please post or email. thank you! - matt -- | matthew b. lawson <------------> | --+-- \"now i, nebuchadnezzar, praise and exalt and glor\n",
      "\n",
      "Length: 963 → 545\n",
      "\n",
      "--- Document 2 (Category: talk.politics.mideast) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stre\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "|>the student of \"regional killings\" alias davidian (not the davidian religios sect) writes: |>greater armenia would stretch from karabakh, to the black sea, to the |>mediterranean, so if you use the term \"greater armenia\" use it with care. finally you said what you dream about. mediterranean???? that was new.... the area will be \"greater\" after some years, like your \"holocaust\" numbers...... |>it\n",
      "\n",
      "Length: 3780 → 3282\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET\n",
      "======================================================================\n",
      "Original documents: 18846\n",
      "After cleaning and filtering (min_length=50): 18682\n",
      "Removed: 164 documents\n",
      "\n",
      "Final shape: (18682, 3)\n",
      "\n",
      "Sample of final dataset:\n",
      "   document_id                                               text  \\\n",
      "0            0  i am sure some bashers of pens fans are pretty...   \n",
      "1            1  my brother is in the market for a high-perform...   \n",
      "2            2  |>the student of \"regional killings\" alias dav...   \n",
      "3            3  in article (wayne smith) writes: think! it's t...   \n",
      "4            4  1) i have an old jasmine drive which i cannot ...   \n",
      "\n",
      "                 true_label  \n",
      "0          rec.sport.hockey  \n",
      "1  comp.sys.ibm.pc.hardware  \n",
      "2     talk.politics.mideast  \n",
      "3  comp.sys.ibm.pc.hardware  \n",
      "4     comp.sys.mac.hardware  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_newsgroup_text(text):\n",
    "    \"\"\"Clean newsgroup text based on identified patterns.\"\"\"\n",
    "    \n",
    "    # 1. Remove email headers (lines starting with common patterns)\n",
    "    # Common headers: From:, Subject:, Organization:, Lines:, NNTP-Posting-Host:, etc.\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    in_header = True\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip header lines (lines with key: value format at the start)\n",
    "        if in_header and re.match(r'^[\\w-]+:', line):\n",
    "            continue\n",
    "        elif in_header and line.strip() == '':\n",
    "            # Empty line often marks end of headers\n",
    "            in_header = False\n",
    "            continue\n",
    "        else:\n",
    "            in_header = False\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # 2. Remove quoted text (lines starting with > or >>)\n",
    "    text = re.sub(r'^>+.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove URLs (minimal presence but good to clean)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 5. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 6. Normalize whitespace (replace multiple spaces/newlines with single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Applying cleaning to all documents...\")\n",
    "df['text_cleaned'] = df['text'].apply(clean_newsgroup_text)\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate new text lengths\n",
    "df['cleaned_length'] = df['text_cleaned'].str.len()\n",
    "\n",
    "print(\"\\nText length comparison:\")\n",
    "print(\"Original:\")\n",
    "print(df['text_length'].describe())\n",
    "print(\"\\nCleaned:\")\n",
    "print(df['cleaned_length'].describe())\n",
    "\n",
    "# Check for very short documents after cleaning\n",
    "print(f\"\\nDocuments with < 50 characters after cleaning: {(df['cleaned_length'] < 50).sum()}\")\n",
    "print(f\"Documents with < 100 characters after cleaning: {(df['cleaned_length'] < 100).sum()}\")\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEFORE/AFTER EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    print(f\"\\n--- Document {i} (Category: {df['true_label'].iloc[i]}) ---\")\n",
    "    print(f\"\\nORIGINAL (first 400 chars):\")\n",
    "    print(df['text'].iloc[i][:400])\n",
    "    print(f\"\\nCLEANED (first 400 chars):\")\n",
    "    print(df['text_cleaned'].iloc[i][:400])\n",
    "    print(f\"\\nLength: {df['text_length'].iloc[i]} → {df['cleaned_length'].iloc[i]}\")\n",
    "\n",
    "# Filter out very short documents (optional - set threshold)\n",
    "min_length = 50\n",
    "df_final = df[df['cleaned_length'] >= min_length].copy()\n",
    "\n",
    "# Reset document IDs and keep only necessary columns\n",
    "df_final = df_final[['true_label', 'text_cleaned']].reset_index(drop=True)\n",
    "df_final['document_id'] = range(len(df_final))\n",
    "df_final = df_final[['document_id', 'text_cleaned', 'true_label']]\n",
    "df_final.rename(columns={'text_cleaned': 'text'}, inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original documents: {len(df)}\")\n",
    "print(f\"After cleaning and filtering (min_length={min_length}): {len(df_final)}\")\n",
    "print(f\"Removed: {len(df) - len(df_final)} documents\")\n",
    "print(f\"\\nFinal shape: {df_final.shape}\")\n",
    "print(f\"\\nSample of final dataset:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ec04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

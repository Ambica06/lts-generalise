{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144502ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18846 documents\n",
      "\n",
      "Dataset shape: (18846, 3)\n",
      "\n",
      "Categories (20): ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Class distribution:\n",
      "true_label\n",
      "alt.atheism                 799\n",
      "comp.graphics               973\n",
      "comp.os.ms-windows.misc     985\n",
      "comp.sys.ibm.pc.hardware    982\n",
      "comp.sys.mac.hardware       963\n",
      "comp.windows.x              988\n",
      "misc.forsale                975\n",
      "rec.autos                   990\n",
      "rec.motorcycles             996\n",
      "rec.sport.baseball          994\n",
      "rec.sport.hockey            999\n",
      "sci.crypt                   991\n",
      "sci.electronics             984\n",
      "sci.med                     990\n",
      "sci.space                   987\n",
      "soc.religion.christian      997\n",
      "talk.politics.guns          910\n",
      "talk.politics.mideast       940\n",
      "talk.politics.misc          775\n",
      "talk.religion.misc          628\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n",
      "   document_id                                               text  \\\n",
      "0            0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...   \n",
      "1            1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...   \n",
      "2            2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...   \n",
      "3            3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...   \n",
      "4            4  From: Alexander Samuel McDiarmid <am2o+@andrew...   \n",
      "\n",
      "                 true_label  \n",
      "0          rec.sport.hockey  \n",
      "1  comp.sys.ibm.pc.hardware  \n",
      "2     talk.politics.mideast  \n",
      "3  comp.sys.ibm.pc.hardware  \n",
      "4     comp.sys.mac.hardware  \n",
      "\n",
      "Sample text from first document:\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killin\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Download the complete dataset (both train and test)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'document_id': range(len(newsgroups.data)),\n",
    "    'text': newsgroups.data,\n",
    "    'true_label': [newsgroups.target_names[label] for label in newsgroups.target]\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nCategories ({len(df['true_label'].unique())}): {sorted(df['true_label'].unique())}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['true_label'].value_counts().sort_index())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nSample text from first document:\")\n",
    "print(df['text'].iloc[0][:500])  # First 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6400d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA EXPLORATION AND INCONSISTENCY CHECK\n",
      "======================================================================\n",
      "\n",
      "1. BASIC STATISTICS:\n",
      "Total documents: 18846\n",
      "Number of categories: 20\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "2. NULL/EMPTY VALUES:\n",
      "document_id    0\n",
      "text           0\n",
      "true_label     0\n",
      "dtype: int64\n",
      "Empty strings: 0\n",
      "\n",
      "3. TEXT LENGTH DISTRIBUTION:\n",
      "count     18846.000000\n",
      "mean       1902.525894\n",
      "std        3984.970264\n",
      "min         115.000000\n",
      "25%         751.000000\n",
      "50%        1175.000000\n",
      "75%        1874.750000\n",
      "max      160616.000000\n",
      "Name: text_length, dtype: float64\n",
      "Documents with < 50 characters: 0\n",
      "Documents with < 100 characters: 0\n",
      "\n",
      "4. SAMPLE DOCUMENTS (First 3):\n",
      "\n",
      "--- Document 0 (Category: rec.sport.hockey) ---\n",
      "Length: 902 characters\n",
      "First 800 characters:\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was \n",
      "...\n",
      "\n",
      "--- Document 1 (Category: comp.sys.ibm.pc.hardware) ---\n",
      "Length: 963 characters\n",
      "First 800 characters:\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "-- \n",
      "    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \n",
      "  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  \n",
      "...\n",
      "\n",
      "--- Document 2 (Category: talk.politics.mideast) ---\n",
      "Length: 3780 characters\n",
      "First 800 characters:\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the\n",
      "|>Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>It has always been up to the Azeris to end their announced winning of Karabakh \n",
      "|>by removing the Armenians! When t\n",
      "...\n",
      "\n",
      "5. PATTERNS DETECTED:\n",
      "Documents with email headers: 18749 (99.5%)\n",
      "Documents with quoted text (>): 9486 (50.3%)\n",
      "Documents with email addresses: 18836 (99.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/d7nhbf657dq4jbk0t91qjchc0000gn/T/ipykernel_58251/3207241922.py:38: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  has_headers = df['text'].str.contains(r'^(From|Subject|Organization):', case=False, regex=True, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with URLs: 3 (0.0%)\n",
      "Documents with excessive whitespace: 15945 (84.6%)\n",
      "Documents with special characters: 18843 (100.0%)\n",
      "\n",
      "6. CLASS DISTRIBUTION:\n",
      "true_label\n",
      "alt.atheism                 799\n",
      "comp.graphics               973\n",
      "comp.os.ms-windows.misc     985\n",
      "comp.sys.ibm.pc.hardware    982\n",
      "comp.sys.mac.hardware       963\n",
      "comp.windows.x              988\n",
      "misc.forsale                975\n",
      "rec.autos                   990\n",
      "rec.motorcycles             996\n",
      "rec.sport.baseball          994\n",
      "rec.sport.hockey            999\n",
      "sci.crypt                   991\n",
      "sci.electronics             984\n",
      "sci.med                     990\n",
      "sci.space                   987\n",
      "soc.religion.christian      997\n",
      "talk.politics.guns          910\n",
      "talk.politics.mideast       940\n",
      "talk.politics.misc          775\n",
      "talk.religion.misc          628\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most common: 999 documents\n",
      "Least common: 628 documents\n",
      "Balance ratio (max/min): 1.59\n",
      "\n",
      "======================================================================\n",
      "EXPLORATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA EXPLORATION AND INCONSISTENCY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Basic statistics\n",
    "print(\"\\n1. BASIC STATISTICS:\")\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"Number of categories: {df['true_label'].nunique()}\")\n",
    "print(f\"Categories: {sorted(df['true_label'].unique())}\")\n",
    "\n",
    "# 2. Check for null/empty values\n",
    "print(\"\\n2. NULL/EMPTY VALUES:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"Empty strings: {(df['text'].str.strip() == '').sum()}\")\n",
    "\n",
    "# 3. Text length distribution\n",
    "print(\"\\n3. TEXT LENGTH DISTRIBUTION:\")\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(df['text_length'].describe())\n",
    "print(f\"Documents with < 50 characters: {(df['text_length'] < 50).sum()}\")\n",
    "print(f\"Documents with < 100 characters: {(df['text_length'] < 100).sum()}\")\n",
    "\n",
    "# 4. Sample a few documents to see their structure\n",
    "print(\"\\n4. SAMPLE DOCUMENTS (First 3):\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- Document {i} (Category: {df['true_label'].iloc[i]}) ---\")\n",
    "    print(f\"Length: {df['text_length'].iloc[i]} characters\")\n",
    "    print(f\"First 800 characters:\")\n",
    "    print(df['text'].iloc[i][:800])\n",
    "    print(\"...\")\n",
    "\n",
    "# 5. Check for common patterns that need cleaning\n",
    "print(\"\\n5. PATTERNS DETECTED:\")\n",
    "\n",
    "# Check for email headers\n",
    "has_headers = df['text'].str.contains(r'^(From|Subject|Organization):', case=False, regex=True, na=False)\n",
    "print(f\"Documents with email headers: {has_headers.sum()} ({has_headers.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for quoted text\n",
    "has_quotes = df['text'].str.contains(r'^>+', regex=True, flags=re.MULTILINE, na=False)\n",
    "print(f\"Documents with quoted text (>): {has_quotes.sum()} ({has_quotes.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for email addresses\n",
    "has_emails = df['text'].str.contains(r'\\S+@\\S+', regex=True, na=False)\n",
    "print(f\"Documents with email addresses: {has_emails.sum()} ({has_emails.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for URLs\n",
    "has_urls = df['text'].str.contains(r'http\\S+|www\\.\\S+', regex=True, na=False)\n",
    "print(f\"Documents with URLs: {has_urls.sum()} ({has_urls.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for excessive whitespace\n",
    "has_excess_space = df['text'].str.contains(r'\\s{3,}', regex=True, na=False)\n",
    "print(f\"Documents with excessive whitespace: {has_excess_space.sum()} ({has_excess_space.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for special characters\n",
    "has_special_chars = df['text'].str.contains(r'[^\\w\\s.,!?;:\\'\"()-]', regex=True, na=False)\n",
    "print(f\"Documents with special characters: {has_special_chars.sum()} ({has_special_chars.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 6. Class balance\n",
    "print(\"\\n6. CLASS DISTRIBUTION:\")\n",
    "class_counts = df['true_label'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print(f\"\\nMost common: {class_counts.max()} documents\")\n",
    "print(f\"Least common: {class_counts.min()} documents\")\n",
    "print(f\"Balance ratio (max/min): {class_counts.max()/class_counts.min():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2d6bdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying cleaning to all documents...\n",
      "\n",
      "======================================================================\n",
      "CLEANING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Text length comparison:\n",
      "Original:\n",
      "count     18846.000000\n",
      "mean       1902.525894\n",
      "std        3984.970264\n",
      "min         115.000000\n",
      "25%         751.000000\n",
      "50%        1175.000000\n",
      "75%        1874.750000\n",
      "max      160616.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Cleaned:\n",
      "count    18846.000000\n",
      "mean      1262.492041\n",
      "std       3367.285109\n",
      "min          0.000000\n",
      "25%        359.000000\n",
      "50%        627.500000\n",
      "75%       1143.750000\n",
      "max      82312.000000\n",
      "Name: cleaned_length, dtype: float64\n",
      "\n",
      "Documents with < 50 characters after cleaning: 164\n",
      "Documents with < 100 characters after cleaning: 533\n",
      "\n",
      "======================================================================\n",
      "BEFORE/AFTER EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "--- Document 0 (Category: rec.sport.hockey) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am goin\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "i am sure some bashers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils. actually, i am bit puzzled too and a bit relieved. however, i am going to put an end to non-pittsburghers' relief with a bit of praise for the pens. man, they are killing those devils worse than i thought. jagr just showed you why he is much better than his regu\n",
      "\n",
      "Length: 902 → 698\n",
      "\n",
      "--- Document 1 (Category: comp.sys.ibm.pc.hardware) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VE\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "my brother is in the market for a high-performance video card that supports vesa local bus with 1-2mb ram. does anyone have suggestions/ideas on: - diamond stealth pro local bus - orchid farenheit 1280 - ati graphics ultra pro - any other high-performance vlb card please post or email. thank you! - matt -- | matthew b. lawson <------------> | --+-- \"now i, nebuchadnezzar, praise and exalt and glor\n",
      "\n",
      "Length: 963 → 545\n",
      "\n",
      "--- Document 2 (Category: talk.politics.mideast) ---\n",
      "\n",
      "ORIGINAL (first 400 chars):\n",
      "From: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\n",
      "Lines: 95\n",
      "Nntp-Posting-Host: viktoria.dsv.su.se\n",
      "Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)\n",
      "Organization: Dept. of Computer and Systems Sciences, Stockholm University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\n",
      "\n",
      "\n",
      "|>Greater Armenia would stre\n",
      "\n",
      "CLEANED (first 400 chars):\n",
      "|>the student of \"regional killings\" alias davidian (not the davidian religios sect) writes: |>greater armenia would stretch from karabakh, to the black sea, to the |>mediterranean, so if you use the term \"greater armenia\" use it with care. finally you said what you dream about. mediterranean???? that was new.... the area will be \"greater\" after some years, like your \"holocaust\" numbers...... |>it\n",
      "\n",
      "Length: 3780 → 3282\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET\n",
      "======================================================================\n",
      "Original documents: 18846\n",
      "After cleaning and filtering (min_length=50): 18682\n",
      "Removed: 164 documents\n",
      "\n",
      "Final shape: (18682, 3)\n",
      "\n",
      "Sample of final dataset:\n",
      "   document_id                                               text  \\\n",
      "0            0  i am sure some bashers of pens fans are pretty...   \n",
      "1            1  my brother is in the market for a high-perform...   \n",
      "2            2  |>the student of \"regional killings\" alias dav...   \n",
      "3            3  in article (wayne smith) writes: think! it's t...   \n",
      "4            4  1) i have an old jasmine drive which i cannot ...   \n",
      "\n",
      "                 true_label  \n",
      "0          rec.sport.hockey  \n",
      "1  comp.sys.ibm.pc.hardware  \n",
      "2     talk.politics.mideast  \n",
      "3  comp.sys.ibm.pc.hardware  \n",
      "4     comp.sys.mac.hardware  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_newsgroup_text(text):\n",
    "    \"\"\"Clean newsgroup text based on identified patterns.\"\"\"\n",
    "    \n",
    "    # 1. Remove email headers (lines starting with common patterns)\n",
    "    # Common headers: From:, Subject:, Organization:, Lines:, NNTP-Posting-Host:, etc.\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    in_header = True\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip header lines (lines with key: value format at the start)\n",
    "        if in_header and re.match(r'^[\\w-]+:', line):\n",
    "            continue\n",
    "        elif in_header and line.strip() == '':\n",
    "            # Empty line often marks end of headers\n",
    "            in_header = False\n",
    "            continue\n",
    "        else:\n",
    "            in_header = False\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # 2. Remove quoted text (lines starting with > or >>)\n",
    "    text = re.sub(r'^>+.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove URLs (minimal presence but good to clean)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 5. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 6. Normalize whitespace (replace multiple spaces/newlines with single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Applying cleaning to all documents...\")\n",
    "df['text_cleaned'] = df['text'].apply(clean_newsgroup_text)\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate new text lengths\n",
    "df['cleaned_length'] = df['text_cleaned'].str.len()\n",
    "\n",
    "print(\"\\nText length comparison:\")\n",
    "print(\"Original:\")\n",
    "print(df['text_length'].describe())\n",
    "print(\"\\nCleaned:\")\n",
    "print(df['cleaned_length'].describe())\n",
    "\n",
    "# Check for very short documents after cleaning\n",
    "print(f\"\\nDocuments with < 50 characters after cleaning: {(df['cleaned_length'] < 50).sum()}\")\n",
    "print(f\"Documents with < 100 characters after cleaning: {(df['cleaned_length'] < 100).sum()}\")\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEFORE/AFTER EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    print(f\"\\n--- Document {i} (Category: {df['true_label'].iloc[i]}) ---\")\n",
    "    print(f\"\\nORIGINAL (first 400 chars):\")\n",
    "    print(df['text'].iloc[i][:400])\n",
    "    print(f\"\\nCLEANED (first 400 chars):\")\n",
    "    print(df['text_cleaned'].iloc[i][:400])\n",
    "    print(f\"\\nLength: {df['text_length'].iloc[i]} → {df['cleaned_length'].iloc[i]}\")\n",
    "\n",
    "# Filter out very short documents (optional - set threshold)\n",
    "min_length = 50\n",
    "df_final = df[df['cleaned_length'] >= min_length].copy()\n",
    "\n",
    "# Reset document IDs and keep only necessary columns\n",
    "df_final = df_final[['true_label', 'text_cleaned']].reset_index(drop=True)\n",
    "df_final['document_id'] = range(len(df_final))\n",
    "df_final = df_final[['document_id', 'text_cleaned', 'true_label']]\n",
    "df_final.rename(columns={'text_cleaned': 'text'}, inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original documents: {len(df)}\")\n",
    "print(f\"After cleaning and filtering (min_length={min_length}): {len(df_final)}\")\n",
    "print(f\"Removed: {len(df) - len(df_final)} documents\")\n",
    "print(f\"\\nFinal shape: {df_final.shape}\")\n",
    "print(f\"\\nSample of final dataset:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596ec04b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load a pre-trained model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and efficient\n",
    "# Or: model = SentenceTransformer('all-mpnet-base-v2')  # More accurate but slower\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "embeddings = model.encode(\n",
    "    df['text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    device='cpu'  \n",
    ")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('document_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666aeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-means with k=100 clusters...\n",
      "Original dataset size: 18846\n",
      "Sampled dataset size: 100\n",
      "Reduction: 0.5%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your embeddings (if saved)\n",
    "embeddings = np.load('document_embeddings.npy')\n",
    "\n",
    "# Choose number of clusters (k)\n",
    "# Common range: 200-500, or you could use k = number of categories × multiplier\n",
    "k = 100  # Adjust based on your dataset size\n",
    "\n",
    "# Run k-means clustering\n",
    "print(f\"Running k-means with k={k} clusters...\")\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k,\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster assignments to your dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Select representative sample: document closest to each centroid\n",
    "sampled_indices = []\n",
    "\n",
    "for cluster_id in range(k):\n",
    "    # Get all documents in this cluster\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_indices = np.where(cluster_mask)[0]\n",
    "    \n",
    "    if len(cluster_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get embeddings for this cluster\n",
    "    cluster_embeddings = embeddings[cluster_indices]\n",
    "    centroid = kmeans.cluster_centers_[cluster_id]\n",
    "    \n",
    "    # Find document closest to centroid\n",
    "    distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
    "    closest_idx = cluster_indices[np.argmin(distances)]\n",
    "    \n",
    "    sampled_indices.append(closest_idx)\n",
    "\n",
    "# Create sampled subset\n",
    "sampled_df = df.iloc[sampled_indices].copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Sampled dataset size: {len(sampled_df)}\")\n",
    "print(f\"Reduction: {len(sampled_df)/len(df)*100:.1f}%\")\n",
    "\n",
    "# Save the sampled subset\n",
    "sampled_df.to_csv('sampled_documents_lts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d06b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12800</td>\n",
       "      <td>From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>948</td>\n",
       "      <td>someone please buy these books!!!!! i am not a...</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10497</td>\n",
       "      <td>From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>1831</td>\n",
       "      <td>in article () writes: hey, joe -- assuming you...</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9913</td>\n",
       "      <td>From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>2303</td>\n",
       "      <td>(joseph b stiehm) writes: as if an aluminum st...</td>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6094</td>\n",
       "      <td>From: sfp@lemur.cit.cornell.edu (Sheila Patter...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>2419</td>\n",
       "      <td>in article (anni dozier) writes: |&gt; after read...</td>\n",
       "      <td>2155</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10250</td>\n",
       "      <td>From: glang@slee01.srl.ford.com (Gordon Lang)\\...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>1331</td>\n",
       "      <td>volker voecking wrote: : : hello : : i have pr...</td>\n",
       "      <td>1069</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text  \\\n",
       "0        12800  From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...   \n",
       "1        10497  From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...   \n",
       "2         9913  From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...   \n",
       "3         6094  From: sfp@lemur.cit.cornell.edu (Sheila Patter...   \n",
       "4        10250  From: glang@slee01.srl.ford.com (Gordon Lang)\\...   \n",
       "\n",
       "                 true_label  text_length  \\\n",
       "0              misc.forsale          948   \n",
       "1        talk.politics.guns         1831   \n",
       "2          rec.sport.hockey         2303   \n",
       "3    soc.religion.christian         2419   \n",
       "4  comp.sys.ibm.pc.hardware         1331   \n",
       "\n",
       "                                        text_cleaned  cleaned_length  cluster  \n",
       "0  someone please buy these books!!!!! i am not a...             696        0  \n",
       "1  in article () writes: hey, joe -- assuming you...             578        1  \n",
       "2  (joseph b stiehm) writes: as if an aluminum st...             869        2  \n",
       "3  in article (anni dozier) writes: |> after read...            2155        3  \n",
       "4  volker voecking wrote: : : hello : : i have pr...            1069        4  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# LLM Labelling starts here\n",
    "import pandas as pd\n",
    "sampled_df = pd.read_csv('sampled_documents_lts.csv')\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b40c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# We chose OpenAI, a cloud LLM, to assist us with the LLM labelling\n",
    "# We retrieved the API key from the link: https://platform.openai.com/api-keys\n",
    "import sys\n",
    "\n",
    "# Install/upgrade the OpenAI Python client in the SAME environment Jupyter is using\n",
    "!{sys.executable} -m pip install --quiet \"openai>=1.0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# OpenAI couldn't originally be founded, so I was trying to identify the path for the active environment\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7326f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\r\n",
      "Version: 2.9.0\r\n",
      "Summary: The official Python library for the openai API\r\n",
      "Home-page: https://github.com/openai/openai-python\r\n",
      "Author: \r\n",
      "Author-email: OpenAI <support@openai.com>\r\n",
      "License: Apache-2.0\r\n",
      "Location: /home/kgt238_nyu_edu/.local/lib/python3.11/site-packages\r\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\r\n",
      "Required-by: \r\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip show openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User site-packages: /home/kgt238_nyu_edu/.local/lib/python3.11/site-packages\n",
      "openai in path? False\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Tryingt to get Jupyter-notebook to use the OpenAI LLM model\n",
    "import site, sys\n",
    "\n",
    "# Make sure Python can see packages installed in ~/.local/lib/python3.11/site-packages\n",
    "user_site = site.getusersitepackages()\n",
    "if user_site not in sys.path:\n",
    "    sys.path.append(user_site)\n",
    "\n",
    "print(\"User site-packages:\", user_site)\n",
    "print(\"openai in path?\", any(\"openai\" in p for p in sys.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI version: 2.9.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Testing the import, since it outputs: OpenAI version: 2.9.0, it means that it works now\n",
    "\n",
    "import openai\n",
    "print(\"OpenAI version:\", openai.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3125362",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"INSERT KEY HERE\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_url = \"https://api.openai.com/v1/chat/completions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d180b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def classify_text_openai(text: str, retries: int = 3, delay: float = 2.0) -> dict:\n",
    "    # Clean and truncate extremely long docs\n",
    "    text = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    text = text[:1000]  # optional: truncate to 1000 chars\n",
    "\n",
    "    prompt = f\"\"\"You are a text annotator. Classify the document below.\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "Return ONLY a single JSON object, no extra text, in this exact format:\n",
    "{{\n",
    "  \"topic\": \"...\",\n",
    "  \"intent\": \"...\",\n",
    "  \"sentiment\": \"...\"\n",
    "}}\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        resp = requests.post(api_url, headers=headers, json=body)\n",
    "\n",
    "        # If HTTP-level error (e.g. quota, rate limit)\n",
    "        if resp.status_code != 200:\n",
    "            return {\n",
    "                \"topic\": \"Personal / Culture / Other\",\n",
    "                \"intent\": \"Other\",\n",
    "                \"sentiment\": \"Neutral\",\n",
    "                \"error\": f\"HTTP {resp.status_code}: {resp.text[:200]}\",\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "            content = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        except Exception as e:\n",
    "            # Couldn’t parse the API JSON wrapper\n",
    "            return {\n",
    "                \"topic\": \"Personal / Culture / Other\",\n",
    "                \"intent\": \"Other\",\n",
    "                \"sentiment\": \"Neutral\",\n",
    "                \"error\": f\"API JSON parse error: {e}; raw={resp.text[:200]}\",\n",
    "            }\n",
    "\n",
    "        # Try to parse the model’s message as JSON\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to salvage JSON substring if the model wrapped it in text\n",
    "            start = content.find(\"{\")\n",
    "            end = content.rfind(\"}\")\n",
    "            if start != -1 and end != -1:\n",
    "                try:\n",
    "                    parsed = json.loads(content[start:end+1])\n",
    "                except json.JSONDecodeError:\n",
    "                    if attempt < retries:\n",
    "                        print(f\"Bad JSON, retrying ({attempt}/{retries})...\")\n",
    "                        time.sleep(delay)\n",
    "                        continue\n",
    "                    return {\n",
    "                        \"topic\": \"Personal / Culture / Other\",\n",
    "                        \"intent\": \"Other\",\n",
    "                        \"sentiment\": \"Neutral\",\n",
    "                        \"error\": f\"Could not parse JSON: {content[:200]}\",\n",
    "                    }\n",
    "            else:\n",
    "                if attempt < retries:\n",
    "                    print(f\"No JSON braces, retrying ({attempt}/{retries})...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                return {\n",
    "                    \"topic\": \"Personal / Culture / Other\",\n",
    "                    \"intent\": \"Other\",\n",
    "                    \"sentiment\": \"Neutral\",\n",
    "                    \"error\": f\"No JSON object in: {content[:200]}\",\n",
    "                }\n",
    "\n",
    "        # Checking to maake sure that the keys exist\n",
    "        if all(k in parsed for k in (\"topic\", \"intent\", \"sentiment\")):\n",
    "            return parsed\n",
    "        else:\n",
    "            if attempt < retries:\n",
    "                print(f\"Missing keys, retrying ({attempt}/{retries})...\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            return {\n",
    "                \"topic\": \"Personal / Culture / Other\",\n",
    "                \"intent\": \"Other\",\n",
    "                \"sentiment\": \"Neutral\",\n",
    "                \"error\": f\"Missing keys in parsed JSON: {parsed}\",\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling 100/100...\n",
      "Done! Labeled: 100, Errors: 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "labels_cloud = []\n",
    "error_count_cloud = 0\n",
    "\n",
    "texts = sampled_df[\"text\"].astype(str).tolist()\n",
    "# texts = sampled_df[\"text\"].astype(str).tolist()[:5] # testing with 5 entries first\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Labelling {i+1}/{len(texts)}...\", end=\"\\r\")\n",
    "    result = classify_text_openai(text)\n",
    "    if \"error\" in result:\n",
    "        error_count_cloud += 1\n",
    "        print(result[\"error\"])\n",
    "    labels_cloud.append(result)\n",
    "\n",
    "print(f\"\\nDone! Labeled: {len(labels_cloud)}, Errors: {error_count_cloud}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f7bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved labeled data to: sampled_documents_lts_labeled_openai.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>cluster</th>\n",
       "      <th>topic</th>\n",
       "      <th>intent</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12800</td>\n",
       "      <td>From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>948</td>\n",
       "      <td>someone please buy these books!!!!! i am not a...</td>\n",
       "      <td>696</td>\n",
       "      <td>0</td>\n",
       "      <td>Book Sale</td>\n",
       "      <td>Sell used textbooks</td>\n",
       "      <td>Desperate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10497</td>\n",
       "      <td>From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>1831</td>\n",
       "      <td>in article () writes: hey, joe -- assuming you...</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>Political Accountability</td>\n",
       "      <td>Defend or criticize the actions of President C...</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9913</td>\n",
       "      <td>From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>2303</td>\n",
       "      <td>(joseph b stiehm) writes: as if an aluminum st...</td>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "      <td>Hockey Violence and Player Conduct</td>\n",
       "      <td>Expressing disapproval of a player's behavior ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6094</td>\n",
       "      <td>From: sfp@lemur.cit.cornell.edu (Sheila Patter...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>2419</td>\n",
       "      <td>in article (anni dozier) writes: |&gt; after read...</td>\n",
       "      <td>2155</td>\n",
       "      <td>3</td>\n",
       "      <td>Discussion on the nature of a religious newsgroup</td>\n",
       "      <td>Express dissatisfaction with the group's dynam...</td>\n",
       "      <td>Frustrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10250</td>\n",
       "      <td>From: glang@slee01.srl.ford.com (Gordon Lang)\\...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>1331</td>\n",
       "      <td>volker voecking wrote: : : hello : : i have pr...</td>\n",
       "      <td>1069</td>\n",
       "      <td>4</td>\n",
       "      <td>IDE hard disk configuration</td>\n",
       "      <td>seeking technical assistance</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text  \\\n",
       "0        12800  From: 02106@ravel.udel.edu (Samuel Ross)\\nSubj...   \n",
       "1        10497  From: cdt@sw.stratus.com (C. D. Tavares)\\nSubj...   \n",
       "2         9913  From: drozinst@db.erau.edu (Drozinski Tim)\\nSu...   \n",
       "3         6094  From: sfp@lemur.cit.cornell.edu (Sheila Patter...   \n",
       "4        10250  From: glang@slee01.srl.ford.com (Gordon Lang)\\...   \n",
       "\n",
       "                 true_label  text_length  \\\n",
       "0              misc.forsale          948   \n",
       "1        talk.politics.guns         1831   \n",
       "2          rec.sport.hockey         2303   \n",
       "3    soc.religion.christian         2419   \n",
       "4  comp.sys.ibm.pc.hardware         1331   \n",
       "\n",
       "                                        text_cleaned  cleaned_length  cluster  \\\n",
       "0  someone please buy these books!!!!! i am not a...             696        0   \n",
       "1  in article () writes: hey, joe -- assuming you...             578        1   \n",
       "2  (joseph b stiehm) writes: as if an aluminum st...             869        2   \n",
       "3  in article (anni dozier) writes: |> after read...            2155        3   \n",
       "4  volker voecking wrote: : : hello : : i have pr...            1069        4   \n",
       "\n",
       "                                               topic  \\\n",
       "0                                          Book Sale   \n",
       "1                           Political Accountability   \n",
       "2                 Hockey Violence and Player Conduct   \n",
       "3  Discussion on the nature of a religious newsgroup   \n",
       "4                        IDE hard disk configuration   \n",
       "\n",
       "                                              intent   sentiment  \n",
       "0                                Sell used textbooks   Desperate  \n",
       "1  Defend or criticize the actions of President C...       Mixed  \n",
       "2  Expressing disapproval of a player's behavior ...    Negative  \n",
       "3  Express dissatisfaction with the group's dynam...  Frustrated  \n",
       "4                       seeking technical assistance     neutral  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Save the labeled dataset as a separate csv file\n",
    "\n",
    "sampled_df[\"topic\"] = [r.get(\"topic\", \"Personal / Culture / Other\") for r in labels_cloud]\n",
    "sampled_df[\"intent\"] = [r.get(\"intent\", \"Other\") for r in labels_cloud]\n",
    "sampled_df[\"sentiment\"] = [r.get(\"sentiment\", \"Neutral\") for r in labels_cloud]\n",
    "\n",
    "# Adding the labels to the last columns\n",
    "ordered_cols = [\n",
    "    \"document_id\",\n",
    "    \"text\",\n",
    "    \"true_label\",\n",
    "    \"text_length\",\n",
    "    \"text_cleaned\",\n",
    "    \"cleaned_length\",\n",
    "    \"cluster\",\n",
    "    \"topic\",\n",
    "    \"intent\",\n",
    "    \"sentiment\",\n",
    "]\n",
    "\n",
    "sampled_df = sampled_df[ordered_cols]\n",
    "\n",
    "# Save the output file\n",
    "output_path = \"sampled_documents_lts_labeled_openai.csv\"\n",
    "sampled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved labeled data to: {output_path}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75587cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: load full data, labeled data, and embeddings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "EMB_PATH = \"document_embeddings.npy\"\n",
    "LABELED_CSV_PATH = \"sampled_documents_lts_labeled_openai.csv\"  # OpenAI-labeled subset\n",
    "FULL_CSV_PATH = \"sampled_documents_lts.csv\"                    # full dataset\n",
    "\n",
    "# Load data\n",
    "embeddings = np.load(EMB_PATH)\n",
    "labeled_df = pd.read_csv(LABELED_CSV_PATH)\n",
    "full_df = pd.read_csv(FULL_CSV_PATH)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Labeled subset shape:\", labeled_df.shape)\n",
    "print(\"Full dataset shape:\", full_df.shape)\n",
    "\n",
    "# Training set = docs with OpenAI labels\n",
    "train_ids = labeled_df[\"document_id\"].values\n",
    "X_train = embeddings[train_ids]\n",
    "\n",
    "# Test set = entire data - labeled data\n",
    "test_df = full_df[~full_df[\"document_id\"].isin(train_ids)].copy()\n",
    "test_ids = test_df[\"document_id\"].values\n",
    "X_test = embeddings[test_ids]\n",
    "\n",
    "print(\"\\n=== Dataset stats ===\")\n",
    "print(\"Training docs (OpenAI-labeled):\", X_train.shape[0])\n",
    "print(\"Test docs (unlabeled):        \", X_test.shape[0])\n",
    "\n",
    "print(\"\\nSample labeled rows (training data):\")\n",
    "display(labeled_df.head())\n",
    "\n",
    "print(\"\\nSample unlabeled rows (test data):\")\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_openai_label(target_column: str):\n",
    "    \"\"\"\n",
    "    Train on ALL OpenAI-labeled rows (no split),\n",
    "    then predict this label for ALL unlabeled rows.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing target: {target_column}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if target_column not in labeled_df.columns:\n",
    "        print(f\"Column '{target_column}' not found in labeled_df, skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Training labels from OpenAI-labeled subset\n",
    "    y_train = labeled_df[target_column].values\n",
    "\n",
    "    print(\"Training size:\", len(y_train))\n",
    "    print(\"Unique labels:\", labeled_df[target_column].nunique())\n",
    "    print(\"\\nLabel distribution (top 10):\")\n",
    "    print(labeled_df[target_column].value_counts().head(10))\n",
    "\n",
    "    # Simple, fast classifier\n",
    "    model = LinearSVC()\n",
    "\n",
    "    print(\"\\nFitting model on ALL OpenAI-labeled data...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Predicting for ALL unlabeled documents...\")\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    preds_df = pd.DataFrame(\n",
    "        {\n",
    "            \"document_id\": test_ids,\n",
    "            f\"predicted_{target_column}\": test_preds,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPrediction distribution on test set (top 10) for {target_column}:\")\n",
    "    print(preds_df[f\"predicted_{target_column}\"].value_counts().head(10))\n",
    "\n",
    "    print(\"\\nSample predictions with text:\")\n",
    "    joined = (\n",
    "        test_df.merge(preds_df, on=\"document_id\", how=\"inner\")\n",
    "        .loc[:, [\"document_id\", \"text\", f\"predicted_{target_column}\"]]\n",
    "        .head(5)\n",
    "    )\n",
    "    display(joined)\n",
    "\n",
    "    return preds_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for topic, intent, sentiment and write final unified CSV \n",
    "\n",
    "targets = [\"topic\", \"intent\", \"sentiment\"]\n",
    "prediction_frames = {}\n",
    "\n",
    "print(\"\\nRunning distillation for targets:\", targets)\n",
    "\n",
    "for col in targets:\n",
    "    prediction_frames[col] = train_and_predict_openai_label(col)\n",
    "\n",
    "# Start from the full dataset\n",
    "final_df = full_df.copy()\n",
    "\n",
    "# Attach OpenAI labels (topic/intent/sentiment) only for labeled/training subset\n",
    "final_df = final_df.merge(\n",
    "    labeled_df[[\"document_id\", \"topic\", \"intent\", \"sentiment\"]],\n",
    "    on=\"document_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Attach predictions for each target (only defined for test docs)\n",
    "for col in targets:\n",
    "    preds_df = prediction_frames[col]\n",
    "    final_df = final_df.merge(\n",
    "        preds_df,\n",
    "        on=\"document_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Final dataset sample with OpenAI + predicted labels ===\")\n",
    "display(final_df.head(10))\n",
    "\n",
    "# Save everything into a single CSV\n",
    "output_path = \"sampled_documents_lts_labeled_openai.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n Final unified CSV saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "6049cbf59edd1266bacd55e0ea99195082efb05d5b569daf1a27e61e488ef4ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
